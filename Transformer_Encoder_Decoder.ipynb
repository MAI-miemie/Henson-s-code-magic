{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c7e1a11",
   "metadata": {},
   "source": [
    "## Transformer Encoder çš„æ¯ä¸ª Block ç»“æ„è¯¦è§£\n",
    "\n",
    "1. **å¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼ˆMulti-Head Self-Attentionï¼‰**\n",
    "   - è¾“å…¥åºåˆ—é¦–å…ˆé€šè¿‡çº¿æ€§å˜æ¢ï¼Œåˆ†åˆ«ç”Ÿæˆ Queryï¼ˆæŸ¥è¯¢ï¼‰ã€Keyï¼ˆé”®ï¼‰ã€Valueï¼ˆå€¼ï¼‰ä¸‰ä¸ªçŸ©é˜µã€‚\n",
    "   - æ¯ä¸ªä½ç½®çš„ Query ä¸æ‰€æœ‰ä½ç½®çš„ Key è®¡ç®—ç›¸å…³æ€§åˆ†æ•°ï¼ˆæ³¨æ„åŠ›åˆ†æ•°ï¼‰ï¼Œå¹¶å¯¹ Value åŠ æƒæ±‚å’Œï¼Œå¾—åˆ°æ–°çš„è¡¨ç¤ºã€‚\n",
    "   - å¤šå¤´æœºåˆ¶å°†è¾“å…¥æ‹†åˆ†ä¸ºå¤šä¸ªå­ç©ºé—´ï¼Œåˆ†åˆ«è®¡ç®—æ³¨æ„åŠ›ï¼Œæœ€åæ‹¼æ¥ç»“æœï¼Œå¢å¼ºæ¨¡å‹è¡¨è¾¾èƒ½åŠ›ã€‚\n",
    "\n",
    "2. **æ®‹å·®è¿æ¥ä¸ LayerNorm**\n",
    "   - è‡ªæ³¨æ„åŠ›è¾“å‡ºä¸åŸè¾“å…¥åšæ®‹å·®è¿æ¥ï¼ˆç›¸åŠ ï¼‰ï¼Œå†è¿›è¡Œ Layer Normalizationï¼ˆå±‚å½’ä¸€åŒ–ï¼‰ï¼Œæå‡è®­ç»ƒç¨³å®šæ€§ï¼ŒåŠ é€Ÿæ”¶æ•›ã€‚\n",
    "\n",
    "3. **å‰é¦ˆå…¨è¿æ¥ç½‘ç»œï¼ˆFeed Forward Network, FFNï¼‰**\n",
    "   - æ¯ä¸ªä½ç½®çš„è¾“å‡ºåˆ†åˆ«é€šè¿‡ä¸¤å±‚çº¿æ€§å˜æ¢å’Œæ¿€æ´»å‡½æ•°ï¼ˆå¦‚ ReLU æˆ– GELUï¼‰ï¼Œç»“æ„é€šå¸¸ä¸ºï¼š`Linear -> Activation -> Linear`ã€‚\n",
    "   - è¿›ä¸€æ­¥æå‡æ¯ä¸ªä½ç½®çš„ç‰¹å¾è¡¨è¾¾èƒ½åŠ›ã€‚\n",
    "\n",
    "4. **å†æ¬¡æ®‹å·®è¿æ¥ä¸ LayerNorm**\n",
    "   - FFN è¾“å‡ºä¸è‡ªæ³¨æ„åŠ›åçš„ç»“æœåšæ®‹å·®è¿æ¥ï¼Œå†è¿›è¡Œ Layer Normalizationï¼Œæœ€ç»ˆè¾“å‡ºæ¯ä¸ª block çš„ç»“æœã€‚\n",
    "\n",
    "---\n",
    "\n",
    "**æ•´ä½“ç»“æ„æµç¨‹å›¾ï¼š**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b11e76",
   "metadata": {},
   "source": [
    "è¾“å…¥  \n",
    "â†“  \n",
    "ä½ç½®ç¼–ç (Positional Encoding)  \n",
    "â†“   \n",
    "å¤šå¤´è‡ªæ³¨æ„åŠ›ï¼ˆMulti-Head Self-Attentionï¼‰  \n",
    "â†“  \n",
    "æ®‹å·®è¿æ¥ + LayerNorm  \n",
    "â†“  \n",
    "å‰é¦ˆç½‘ç»œï¼ˆFeed Forward Networkï¼‰  \n",
    "â†“  \n",
    "æ®‹å·®è¿æ¥ + LayerNorm  \n",
    "â†“  \n",
    "è¾“å‡º"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76bcfb0",
   "metadata": {},
   "source": [
    "## ä½ç½®ç¼–ç Positional_Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2646a76f",
   "metadata": {},
   "source": [
    "ä½ è¿™æ®µ `PositionalEncoding` æ˜¯ Transformer **Encoder** é‡Œå¸¸ç”¨çš„**ä½ç½®ç¼–ç ï¼ˆPositional Encodingï¼‰**å®ç°ï¼Œæ ¸å¿ƒä½œç”¨æ˜¯ï¼š\n",
    "ç»™çº¯ç²¹çš„è¯å‘é‡ï¼ˆword embeddingï¼‰åŠ å…¥**ä½ç½®ä¿¡æ¯**ï¼Œè®©æ¨¡å‹èƒ½åŒºåˆ†â€œæˆ‘çˆ±ä½ â€å’Œâ€œä½ çˆ±æˆ‘â€è¿™ç§å•è¯é¡ºåºä¸åŒçš„å¥å­ã€‚\n",
    "\n",
    "ä¸‹é¢æˆ‘æŒ‰ç»“æ„æ‹†å¼€è®²ï¼š\n",
    "\n",
    "---\n",
    "\n",
    "### 1. **ä¸ºä»€ä¹ˆéœ€è¦ä½ç½®ç¼–ç ï¼Ÿ**\n",
    "\n",
    "Transformer çš„ **Self-Attention** æœºåˆ¶æœ¬èº«æ˜¯**é¡ºåºæ— å…³**çš„ï¼Œå®ƒåªçœ‹ token é—´çš„ç›¸ä¼¼åº¦ï¼Œä¸çŸ¥é“è°åœ¨å‰è°åœ¨åã€‚\n",
    "æ‰€ä»¥å¿…é¡»ç»™æ¯ä¸ª token åŠ ä¸€ä¸ª**ä½ç½®å‘é‡**ï¼ˆposition vectorï¼‰ï¼Œè¿™æ ·æ¨¡å‹æ‰çŸ¥é“ token åœ¨å¥å­é‡Œçš„ä½ç½®ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **ä»£ç æ‰§è¡Œæµç¨‹**\n",
    "\n",
    "#### (1) åˆ›å»ºç©ºçŸ©é˜µ\n",
    "\n",
    "```python\n",
    "pe = torch.zeros(max_len, embed_size)\n",
    "```\n",
    "\n",
    "* `max_len`ï¼šå¥å­æœ€å¤§é•¿åº¦ï¼ˆ5000ä¸ªä½ç½®ï¼‰ã€‚\n",
    "* `embed_size`ï¼šè¯å‘é‡ç»´åº¦ã€‚\n",
    "* æ¯è¡Œä»£è¡¨ä¸€ä¸ªä½ç½®ï¼ˆposï¼‰ï¼Œæ¯åˆ—ä»£è¡¨è¯¥ä½ç½®å¯¹åº”çš„ä¸€ä¸ªç»´åº¦ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "#### (2) ç”Ÿæˆä½ç½®ç´¢å¼•\n",
    "\n",
    "```python\n",
    "position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "```\n",
    "\n",
    "* å¾—åˆ° `[0, 1, 2, ..., max_len-1]` çš„åˆ—å‘é‡ï¼Œè¡¨ç¤ºä½ç½® IDã€‚\n",
    "* `unsqueeze(1)` æŠŠå®ƒå˜æˆ `(max_len, 1)` æ–¹ä¾¿å¹¿æ’­è®¡ç®—ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "#### (3) ç”Ÿæˆé¢‘ç‡ç¼©æ”¾å› å­\n",
    "\n",
    "```python\n",
    "div_term = torch.exp(\n",
    "    torch.arange(0, embed_size, 2).float() * (-math.log(10000.0) / embed_size)\n",
    ")\n",
    "```\n",
    "\n",
    "* `torch.arange(0, embed_size, 2)`ï¼šå–å¶æ•°ä¸‹æ ‡ï¼ˆ0, 2, 4...ï¼‰ã€‚\n",
    "* ç¼©æ”¾å› å­å…¬å¼ï¼š\n",
    "\n",
    "  $$\n",
    "  \\frac{1}{10000^{2i / d_{\\text{model}}}}\n",
    "  $$\n",
    "\n",
    "  è¿™æ˜¯è®ºæ–‡ *Attention is All You Need* æå‡ºçš„ï¼Œè®©ä¸åŒç»´åº¦çš„æ³¢å½¢é¢‘ç‡ä¸åŒï¼Œä¾¿äºæ¨¡å‹æ•æ‰çŸ­è·ç¦»å’Œé•¿è·ç¦»çš„ä¾èµ–ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "#### (4) è®¡ç®— sin/cos ä½ç½®ç¼–ç \n",
    "\n",
    "```python\n",
    "pe[:, 0::2] = torch.sin(position * div_term)  # å¶æ•°åˆ—\n",
    "pe[:, 1::2] = torch.cos(position * div_term)  # å¥‡æ•°åˆ—\n",
    "```\n",
    "\n",
    "* **å¶æ•°ç»´**ï¼š`sin(pos / 10000^(2i/d))`\n",
    "* **å¥‡æ•°ç»´**ï¼š`cos(pos / 10000^(2i/d))`\n",
    "* ä¸ºä»€ä¹ˆç”¨ sin/cosï¼Ÿ\n",
    "\n",
    "  * å‘¨æœŸæ€§ â†’ å¯ä»¥æ¨å¹¿åˆ°æ¯”è®­ç»ƒæ—¶æ›´é•¿çš„åºåˆ—ã€‚\n",
    "  * ä¸åŒç»´åº¦æœ‰ä¸åŒå‘¨æœŸ â†’ æ¨¡å‹å¯çº¿æ€§ç»„åˆå¾—åˆ°ä»»ä½•ç›¸å¯¹ä½ç½®ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "#### (5) å½¢çŠ¶è°ƒæ•´\n",
    "\n",
    "```python\n",
    "pe = pe.unsqueeze(0)  # (1, max_len, embed_size)\n",
    "```\n",
    "\n",
    "* å¤šåŠ ä¸€ä¸ª batch ç»´åº¦ï¼Œæ–¹ä¾¿å’Œ `(batch_size, seq_len, embed_size)` çš„è¾“å…¥å¯¹é½ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "#### (6) æ³¨å†Œ buffer\n",
    "\n",
    "```python\n",
    "self.register_buffer('pe', pe)\n",
    "```\n",
    "\n",
    "* `pe` ä¸æ˜¯å‚æ•°ï¼ˆä¸ä¼šè¢«æ¢¯åº¦æ›´æ–°ï¼‰ï¼Œä½†ä¼šéšæ¨¡å‹ä¿å­˜/åŠ è½½ã€‚\n",
    "* é¿å…è¢«è¯¯è®¤ä¸ºæ˜¯å¯è®­ç»ƒæƒé‡ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "#### (7) å‰å‘ä¼ æ’­\n",
    "\n",
    "```python\n",
    "x = x + self.pe[:, :x.size(1), :]\n",
    "```\n",
    "\n",
    "* å–å‡ºå’Œå½“å‰å¥å­é•¿åº¦ `seq_len` åŒ¹é…çš„é‚£ä¸€æ®µä½ç½®ç¼–ç ã€‚\n",
    "* ç›´æ¥ **é€å…ƒç´ ç›¸åŠ ** åˆ°è¯å‘é‡ embedding ä¸Šï¼Œå¾—åˆ°å¸¦ä½ç½®ä¿¡æ¯çš„è¾“å…¥ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **æ•´ä½“è¿‡ç¨‹æ€»ç»“**\n",
    "\n",
    "**è¾“å…¥**ï¼šè¯å‘é‡çŸ©é˜µ `(batch_size, seq_len, embed_size)`\n",
    "**è¾“å‡º**ï¼šåŠ å…¥ä½ç½®ç¼–ç çš„è¯å‘é‡ `(batch_size, seq_len, embed_size)`\n",
    "\n",
    "> åŸç†ä¸Šï¼ŒEncoder åœ¨é€è¿›å¤šå¤´æ³¨æ„åŠ›ä¹‹å‰ï¼Œå…ˆç”¨ï¼š\n",
    ">\n",
    "> $$\n",
    "> x_{\\text{pos}} = x_{\\text{embed}} + PE\n",
    "> $$\n",
    ">\n",
    "> è¿™æ · Attention å°±èƒ½åˆ©ç”¨ç›¸å¯¹ä½ç½®ä¿¡æ¯ã€‚\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b67c8ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "# ä½ç½®ç¼–ç \n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embed_size, max_len=5000):\n",
    "        super().__init__()\n",
    "        # åˆ›å»ºä¸€ä¸ªå…¨é›¶çŸ©é˜µï¼Œå½¢çŠ¶ä¸º (max_len, embed_size)ï¼Œç”¨äºå­˜å‚¨æ¯ä¸ªä½ç½®çš„ç¼–ç \n",
    "        pe = torch.zeros(max_len, embed_size)\n",
    "        # ç”Ÿæˆä½ç½®ç´¢å¼•ï¼Œå½¢çŠ¶ä¸º (max_len, 1)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        # è®¡ç®—æ¯ä¸ªç»´åº¦çš„ç¼©æ”¾å› å­ï¼Œå…¬å¼æ¥è‡ªè®ºæ–‡\n",
    "        div_term = torch.exp(torch.arange(0, embed_size, 2).float() * (-math.log(10000.0) / embed_size))\n",
    "        # å¶æ•°ç»´åº¦ç”¨ sinï¼Œå¥‡æ•°ç»´åº¦ç”¨ cosï¼Œç”Ÿæˆä¸åŒé¢‘ç‡çš„æ³¢å½¢\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        # å¢åŠ  batch ç»´åº¦ï¼Œå˜æˆ (1, max_len, embed_size)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        # æ³¨å†Œä¸º bufferï¼Œä¸å‚ä¸è®­ç»ƒï¼Œä½†ä¼šéšæ¨¡å‹ä¿å­˜\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, seq_len, embed_size)\n",
    "        # å–å‡ºå’Œè¾“å…¥åºåˆ—é•¿åº¦åŒ¹é…çš„é‚£ä¸€æ®µä½ç½®ç¼–ç ï¼Œå¹¶åŠ åˆ°è¾“å…¥ä¸Š\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eaa6fa5",
   "metadata": {},
   "source": [
    "## å¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675fff65",
   "metadata": {},
   "source": [
    "(N, seq_len, embed_size)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;â†“ Q/K æŠ•å½±åˆ° qk_dimï¼ŒV ä¿æŒ embed_size  \n",
    "(N, seq_len, qk_dim) / (N, seq_len, embed_size)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;â†“ å¤šå¤´æ‹†åˆ†  \n",
    "(N, heads, seq_len, head_dim_qk) / (N, heads, seq_len, head_dim_v)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;â†“ æ³¨æ„åŠ›è®¡ç®—  \n",
    "(N, heads, seq_len, head_dim_v)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;â†“ å¤šå¤´åˆå¹¶  \n",
    "(N, seq_len, embed_size)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;â†“ è¾“å‡ºçº¿æ€§å±‚  \n",
    "(N, seq_len, embed_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5683d000",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, embed_size, heads, qk_dim):\n",
    "        \"\"\"\n",
    "        embed_size: è¾“å…¥ token å‘é‡ç»´åº¦ï¼ˆd_modelï¼‰\n",
    "        heads: æ³¨æ„åŠ›å¤´æ•°\n",
    "        qk_dim: Q/K æŠ•å½±åçš„ç»´åº¦ï¼ˆä¼šå‡åˆ†åˆ°æ¯ä¸ªå¤´ï¼‰\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.heads = heads\n",
    "        self.head_dim_qk = qk_dim // heads           # æ¯ä¸ªå¤´çš„ Q/K ç»´åº¦\n",
    "        self.head_dim_v = embed_size // heads        # æ¯ä¸ªå¤´çš„ V ç»´åº¦ï¼ˆä¿æŒå’Œè¾“å…¥ä¸€è‡´ï¼‰\n",
    "\n",
    "        assert qk_dim % heads == 0, \"qk_dim å¿…é¡»èƒ½è¢« heads æ•´é™¤\"\n",
    "        assert embed_size % heads == 0, \"embed_size å¿…é¡»èƒ½è¢« heads æ•´é™¤\"\n",
    "\n",
    "        # Q/K çš„çº¿æ€§æ˜ å°„åˆ° qk_dim\n",
    "        self.query = nn.Linear(embed_size, qk_dim)\n",
    "        self.key = nn.Linear(embed_size, qk_dim)\n",
    "        # V ä¿æŒ embed_size ç»´\n",
    "        self.value = nn.Linear(embed_size, embed_size)\n",
    "\n",
    "        # è¾“å‡ºçº¿æ€§å±‚\n",
    "        self.fc_out = nn.Linear(embed_size, embed_size)\n",
    "\n",
    "    def forward(self, query, key=None, value=None, mask=None):\n",
    "        \"\"\"\n",
    "        query: (N, seq_len_q, embed_size)\n",
    "        key:   (N, seq_len_k, embed_size)ï¼Œé»˜è®¤ Noneï¼Œåˆ™ç”¨ query\n",
    "        value: (N, seq_len_v, embed_size)ï¼Œé»˜è®¤ Noneï¼Œåˆ™ç”¨ query\n",
    "        mask:  æ³¨æ„åŠ› maskï¼Œå¯é€‰\n",
    "        \"\"\"\n",
    "        if key is None:\n",
    "            key = query\n",
    "        if value is None:\n",
    "            value = query\n",
    "\n",
    "        N = query.shape[0]\n",
    "        query_len = query.shape[1]\n",
    "        key_len = key.shape[1]\n",
    "        value_len = value.shape[1]\n",
    "\n",
    "        # 1ï¸âƒ£ Q/K/V æŠ•å½±\n",
    "        Q = self.query(query)    # (N, query_len, qk_dim)\n",
    "        K = self.key(key)        # (N, key_len, qk_dim)\n",
    "        V = self.value(value)    # (N, value_len, embed_size)\n",
    "\n",
    "        # 2ï¸âƒ£ æ‹†æˆå¤šå¤´\n",
    "        Q = Q.view(N, query_len, self.heads, self.head_dim_qk).transpose(1, 2)  # (N, heads, query_len, head_dim_qk)\n",
    "        K = K.view(N, key_len, self.heads, self.head_dim_qk).transpose(1, 2)    # (N, heads, key_len, head_dim_qk)\n",
    "        V = V.view(N, value_len, self.heads, self.head_dim_v).transpose(1, 2)   # (N, heads, value_len, head_dim_v)\n",
    "\n",
    "        # 3ï¸âƒ£ Scaled Dot-Product Attention\n",
    "        energy = torch.matmul(Q, K.transpose(-1, -2)) / math.sqrt(self.head_dim_qk)\n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask == 0, float(\"-1e20\"))\n",
    "\n",
    "        attention = torch.softmax(energy, dim=-1)  # (N, heads, query_len, key_len)\n",
    "\n",
    "        # 4ï¸âƒ£ æ³¨æ„åŠ›åŠ æƒ V\n",
    "        out = torch.matmul(attention, V)  # (N, heads, query_len, head_dim_v)\n",
    "\n",
    "        # 5ï¸âƒ£ å¤šå¤´æ‹¼æ¥\n",
    "        out = out.transpose(1, 2).contiguous().view(N, query_len, self.embed_size)  # (N, query_len, embed_size)\n",
    "\n",
    "        # 6ï¸âƒ£ è¾“å‡ºçº¿æ€§å±‚\n",
    "        out = self.fc_out(out)  # (N, query_len, embed_size)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9124c907",
   "metadata": {},
   "source": [
    "## å‰é¦ˆç¥ç»ç½‘ç»œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45285246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å‰é¦ˆç¥ç»ç½‘ç»œ\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(embed_size, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, embed_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a501a2",
   "metadata": {},
   "source": [
    "## æ¯ä¸ªEndoder Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "661681ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder Block\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, embed_size, heads, qk_dim, ffn_hidden, dropout=0.1):\n",
    "        super().__init__()\n",
    "        # ä½¿ç”¨é€šç”¨ SelfAttention\n",
    "        self.attention = SelfAttention(embed_size, heads, qk_dim)\n",
    "        self.norm1 = nn.LayerNorm(embed_size)\n",
    "        self.norm2 = nn.LayerNorm(embed_size)\n",
    "        self.ffn = FeedForward(embed_size, ffn_hidden, dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # 1ï¸âƒ£ è‡ªæ³¨æ„åŠ› + æ®‹å·® + LayerNorm\n",
    "        attn_out = self.attention(query=x, mask=mask)  # Q/K/V éƒ½æ˜¯ x\n",
    "        x = self.norm1(x + self.dropout(attn_out))\n",
    "\n",
    "        # 2ï¸âƒ£ å‰é¦ˆç½‘ç»œ + æ®‹å·® + LayerNorm\n",
    "        ffn_out = self.ffn(x)\n",
    "        x = self.norm2(x + self.dropout(ffn_out))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b55984f",
   "metadata": {},
   "source": [
    "## Transformer Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8601a67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer Encoder\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, num_layers, heads, qk_dim,\n",
    "                 ff_hidden_size, dropout=0.1, max_length=100):\n",
    "        super().__init__()\n",
    "        self.embed_size = embed_size\n",
    "        \n",
    "        # 1ï¸âƒ£ è¯åµŒå…¥\n",
    "        self.word_embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        \n",
    "        # 2ï¸âƒ£ ä½ç½®ç¼–ç \n",
    "        self.position_encoding = PositionalEncoding(embed_size, max_length)\n",
    "        \n",
    "        # 3ï¸âƒ£ å¤šå±‚ EncoderBlock\n",
    "        self.layers = nn.ModuleList([\n",
    "            EncoderBlock(embed_size, heads, qk_dim, ff_hidden_size, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # 4ï¸âƒ£ è¾“å…¥ dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # x: (N, seq_len)\n",
    "        out = self.word_embedding(x)           # (N, seq_len, embed_size)\n",
    "        out = self.position_encoding(out)      # åŠ å…¥ä½ç½®ä¿¡æ¯\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        # å †å å¤šå±‚ EncoderBlock\n",
    "        for layer in self.layers:\n",
    "            out = layer(out, mask)             # (N, seq_len, embed_size)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498191d3",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "`vocab_size` å’Œ `max_length` çœ‹èµ·æ¥éƒ½æ˜¯æ•´æ•°å‚æ•°ï¼Œä½†å®ƒä»¬ä»£è¡¨çš„æ„ä¹‰å®Œå…¨ä¸åŒï¼š\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ”¹ `vocab_size`\n",
    "\n",
    "* **å®šä¹‰**ï¼šè¯è¡¨å¤§å°ï¼Œå³æ¨¡å‹èƒ½è¯†åˆ«çš„ **ä¸åŒ tokenï¼ˆè¯/å­è¯/å­—ç¬¦ï¼‰æ•°é‡**ã€‚\n",
    "* **ä½œç”¨**ï¼šä½œä¸º `nn.Embedding(vocab_size, embed_size)` çš„è¾“å…¥ç»´åº¦ã€‚\n",
    "\n",
    "  * æ¯ä¸ª token ID æ˜¯ `[0, vocab_size-1]` çš„æ•´æ•°ç´¢å¼•\n",
    "  * Embedding å±‚ä¼šæŠŠ ID æ˜ å°„ä¸ºä¸€ä¸ª `embed_size` ç»´çš„å‘é‡\n",
    "\n",
    "ğŸ‘‰ ä¸¾ä¾‹ï¼š\n",
    "\n",
    "* å¦‚æœ `vocab_size = 30,000`ï¼Œè¯´æ˜æ¨¡å‹è¯è¡¨é‡Œæœ‰ 30k ä¸ªä¸åŒçš„ tokenã€‚\n",
    "* è¾“å…¥ token ID `42`ï¼ŒEmbedding ä¼šæŸ¥è¡¨å¾—åˆ° `word_embedding[42]` è¿™ä¸ªå‘é‡ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ”¹ `max_length`\n",
    "\n",
    "* **å®šä¹‰**ï¼šæ¨¡å‹æ”¯æŒçš„ **åºåˆ—æœ€å¤§é•¿åº¦**ï¼Œé€šå¸¸ç”¨äº **ä½ç½®ç¼–ç  (Positional Encoding)**ã€‚\n",
    "* **ä½œç”¨**ï¼šå‘Šè¯‰æ¨¡å‹â€œæˆ‘æœ€å¤šå¤„ç†å¤šé•¿çš„è¾“å…¥åºåˆ—â€ã€‚\n",
    "\n",
    "  * åœ¨ Transformer é‡Œï¼Œä½ç½®ç¼–ç æ˜¯ä¸€ä¸ª `max_length Ã— embed_size` çš„çŸ©é˜µã€‚\n",
    "  * è¾“å…¥åºåˆ—é•¿åº¦ä¸èƒ½è¶…è¿‡ `max_length`ï¼Œå¦åˆ™å°±æ²¡æœ‰å¯¹åº”çš„ä½ç½®ç¼–ç ã€‚\n",
    "\n",
    "ğŸ‘‰ ä¸¾ä¾‹ï¼š\n",
    "\n",
    "* å¦‚æœ `max_length = 100`ï¼Œè¡¨ç¤ºè¾“å…¥/è¾“å‡ºåºåˆ—æœ€é•¿ä¸èƒ½è¶…è¿‡ 100 ä¸ª tokenã€‚\n",
    "* å½“è¾“å…¥å¥å­é•¿ 80 æ—¶ï¼Œåªç”¨å‰ 80 ä¸ªä½ç½®ç¼–ç ï¼›å½“è¾“å…¥å¥å­é•¿ 120 æ—¶ï¼Œå°±ä¼šæŠ¥é”™ï¼ˆè¶…å‡ºæœ€å¤§é•¿åº¦ï¼‰ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "### âš–ï¸ åŒºåˆ«æ€»ç»“\n",
    "\n",
    "| å‚æ•°               | æ§åˆ¶ä»€ä¹ˆ                  | å†³å®šçš„é™åˆ¶                |\n",
    "| ---------------- | --------------------- | -------------------- |\n",
    "| **`vocab_size`** | è¯è¡¨çš„å¹¿åº¦ï¼ˆèƒ½è¯†åˆ«å¤šå°‘ä¸ªä¸åŒ tokenï¼‰ | ä¸èƒ½è¾“å…¥è¶…å‡ºè¯è¡¨èŒƒå›´çš„ token id |\n",
    "| **`max_length`** | åºåˆ—çš„æ·±åº¦ï¼ˆä¸€æ¬¡èƒ½å¤„ç†å¤šå°‘ä¸ª tokenï¼‰ | ä¸èƒ½è¾“å…¥è¶…è¿‡è¯¥é•¿åº¦çš„åºåˆ—         |\n",
    "\n",
    "---\n",
    "\n",
    "#### ç±»æ¯”ç†è§£ ğŸ¯\n",
    "\n",
    "* `vocab_size` åƒ **å­—å…¸é‡Œæœ‰å¤šå°‘ä¸ªè¯**\n",
    "* `max_length` åƒ **ä¸€å¥è¯æœ€å¤šèƒ½æœ‰å¤šå°‘ä¸ªè¯**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d300fded",
   "metadata": {},
   "source": [
    "å¥½çš„ï¼Œæˆ‘ä»¬æ¥ä»”ç»†æ‹†è§£è¿™ä¸€è¡Œä»£ç ï¼š\n",
    "\n",
    "```python\n",
    "self.word_embedding = nn.Embedding(vocab_size, embed_size)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ”¹ 1. `nn.Embedding` çš„ä½œç”¨\n",
    "\n",
    "* å®ƒçš„æœ¬è´¨æ˜¯ä¸€ä¸ª **æŸ¥è¡¨å±‚ (lookup table)**ã€‚\n",
    "* è¾“å…¥ï¼šæ•´æ•°ç´¢å¼• (token id)ï¼ŒèŒƒå›´æ˜¯ `[0, vocab_size-1]`ã€‚\n",
    "* è¾“å‡ºï¼šå¯¹åº”çš„ **ç¨ å¯†å‘é‡** (embedding)ï¼Œç»´åº¦ä¸º `embed_size`ã€‚\n",
    "\n",
    "ä¹Ÿå°±æ˜¯è¯´ï¼š\n",
    "\n",
    "* æ¨¡å‹ä¸ä¼šç›´æ¥å¤„ç†ç¦»æ•£çš„ token idï¼Œè€Œæ˜¯å…ˆæŠŠå®ƒè½¬æˆè¿ç»­çš„å‘é‡ã€‚\n",
    "* è¿™ä¸ªå‘é‡èƒ½æ•æ‰åˆ° token çš„è¯­ä¹‰ä¿¡æ¯ï¼ˆåœ¨è®­ç»ƒä¸­å­¦ä¹ åˆ°ï¼‰ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ”¹ 2. è¾“å…¥å’Œè¾“å‡ºå½¢çŠ¶\n",
    "\n",
    "å‡è®¾å®šä¹‰ï¼š\n",
    "\n",
    "```python\n",
    "word_embedding = nn.Embedding(vocab_size=10000, embed_size=256)\n",
    "```\n",
    "\n",
    "#### è¾“å…¥ (token id åºåˆ—)\n",
    "\n",
    "* è¾“å…¥é€šå¸¸æ˜¯ä¸€ä¸ª `LongTensor`ï¼Œå½¢çŠ¶ï¼š\n",
    "\n",
    "  ```\n",
    "  (N, seq_len)\n",
    "  ```\n",
    "\n",
    "  * `N`: batch size\n",
    "  * `seq_len`: åºåˆ—é•¿åº¦\n",
    "\n",
    "ä¸¾ä¾‹ï¼š\n",
    "\n",
    "```python\n",
    "x = torch.tensor([[2, 45, 78], [5, 9, 100]])  # (N=2, seq_len=3)\n",
    "```\n",
    "\n",
    "è¡¨ç¤ºï¼š\n",
    "\n",
    "* ç¬¬ä¸€ä¸ªæ ·æœ¬åºåˆ— = \\[2, 45, 78]\n",
    "* ç¬¬äºŒä¸ªæ ·æœ¬åºåˆ— = \\[5, 9, 100]\n",
    "\n",
    "---\n",
    "\n",
    "#### è¾“å‡º (embedding å‘é‡åºåˆ—)\n",
    "\n",
    "* è¾“å‡ºçš„å½¢çŠ¶ï¼š\n",
    "\n",
    "  ```\n",
    "  (N, seq_len, embed_size)\n",
    "  ```\n",
    "\n",
    "  * æ¯ä¸ª token id è¢«æ˜ å°„æˆä¸€ä¸ª `embed_size` ç»´çš„å‘é‡\n",
    "\n",
    "ä¸¾ä¾‹ï¼š\n",
    "\n",
    "```python\n",
    "out = word_embedding(x)  # (2, 3, 256)\n",
    "```\n",
    "\n",
    "è¾“å‡ºï¼š\n",
    "\n",
    "* batch é‡Œæœ‰ 2 ä¸ªå¥å­\n",
    "* æ¯ä¸ªå¥å­é•¿åº¦ = 3\n",
    "* æ¯ä¸ª token å˜æˆ 256 ç»´å‘é‡\n",
    "\n",
    "æ‰€ä»¥æœ€ç»ˆ `out` æ˜¯ï¼š\n",
    "\n",
    "```\n",
    "[\n",
    "  [ [e_2], [e_45], [e_78]   ],   # ç¬¬ä¸€ä¸ªå¥å­çš„ embedding åºåˆ—\n",
    "  [ [e_5], [e_9],  [e_100] ]    # ç¬¬äºŒä¸ªå¥å­çš„ embedding åºåˆ—\n",
    "]\n",
    "```\n",
    "\n",
    "å…¶ä¸­ `[e_x]` è¡¨ç¤º token x çš„ embedding å‘é‡ï¼Œç»´åº¦æ˜¯ `256`ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ”¹ 3. ä¸ Transformer çš„å…³ç³»\n",
    "\n",
    "* è¿™æ˜¯ **Encoder/Decoder çš„ç¬¬ä¸€æ­¥**ã€‚\n",
    "* è¾“å…¥ token id â†’ word embedding â†’ åŠ ä¸Š position encoding â†’ é€è¿›åç»­çš„ self-attentionã€‚\n",
    "\n",
    "---\n",
    "\n",
    "âš¡ æ€»ç»“ä¸€å¥ï¼š\n",
    "`nn.Embedding(vocab_size, embed_size)` å°±æ˜¯æŠŠ **ç¦»æ•£çš„å•è¯ id (N, seq\\_len)** è½¬æˆ **ç¨ å¯†çš„è¯­ä¹‰å‘é‡åºåˆ— (N, seq\\_len, embed\\_size)**ï¼Œä¸ºåç»­ Transformer æä¾›è¿ç»­ç©ºé—´çš„è¾“å…¥ã€‚\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037286ab",
   "metadata": {},
   "source": [
    "| å‚æ•°               | å«ä¹‰                                                    |\n",
    "| ---------------- | ----------------------------------------------------- |\n",
    "| `vocab_size`     | è¯è¡¨å¤§å°ï¼Œç”¨äº nn.Embeddingï¼ŒæŠŠ token id æ˜ å°„æˆå‘é‡                 |\n",
    "| `embed_size`     | embedding ç»´åº¦ï¼Œä¹Ÿå°±æ˜¯ d\\_modelï¼Œæ‰€æœ‰ Encoder å±‚è¾“å…¥è¾“å‡ºç»´åº¦ä¸€è‡´        |\n",
    "| `num_layers`     | Encoder å †å çš„å±‚æ•°ï¼Œä¹Ÿå°±æ˜¯æœ‰å¤šå°‘ä¸ª EncoderBlock                    |\n",
    "| `heads`          | å¤šå¤´æ³¨æ„åŠ›å¤´æ•°ï¼Œæ³¨æ„åŠ›æœºåˆ¶ä¼šæŠŠ Q/K/V æ‹†æˆå¤šå¤´                            |\n",
    "| `qk_dim`         | Q/K æŠ•å½±åçš„ç»´åº¦ï¼ˆå¯ä»¥å°äº embed\\_sizeï¼‰ï¼Œæ¯ä¸ªå¤´çš„ç»´åº¦ = qk\\_dim / heads |\n",
    "| `ff_hidden_size` | å‰é¦ˆç½‘ç»œéšè—å±‚ç»´åº¦ï¼Œä¸€èˆ¬æ¯” embed\\_size å¤§ï¼Œæ¯”å¦‚ 4\\~8 å€                 |\n",
    "| `dropout`        | Dropout æ¦‚ç‡ï¼Œç”¨äºé˜²æ­¢è¿‡æ‹Ÿåˆ                                    |\n",
    "| `max_length`     | ä½ç½®ç¼–ç æœ€å¤§æ”¯æŒåºåˆ—é•¿åº¦                                          |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757d0f0a",
   "metadata": {},
   "source": [
    "âœ… ç‰¹ç‚¹ï¼š\n",
    "\n",
    "- x æ˜¯ token id åºåˆ—ï¼Œé€šè¿‡ embedding + position encoding å¾—åˆ°è¾“å…¥å‘é‡ã€‚\n",
    "- æ¯ä¸ª EncoderBlock åŒ…å«ï¼š\n",
    "  - å¤šå¤´è‡ªæ³¨æ„åŠ›ï¼ˆQ/K å¯é™ç»´ï¼ŒV ä¿æŒ embed_sizeï¼‰\n",
    "  - å‰é¦ˆç½‘ç»œ\n",
    "  - æ®‹å·® + LayerNorm\n",
    "- å¯ä»¥å †å å¤šå±‚ï¼Œè¾“å‡ºç»´åº¦å›ºå®šä¸º (batch, seq_len, embed_size)ï¼Œä¾¿äºä¼ å…¥ Decoder æˆ–åšä¸‹æ¸¸ä»»åŠ¡ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea4d14f",
   "metadata": {},
   "source": [
    "## Decoder Block"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947500bb",
   "metadata": {},
   "source": [
    "è¾“å…¥ x  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;â”‚  \n",
    "Masked Self-Attention (Q/K/V = x, mask = tgt_mask)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;â”‚  \n",
    "æ®‹å·® + LayerNorm  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;â”‚  \n",
    "Cross-Attention (Q = x, K/V = enc_out, mask = src_mask)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;â”‚  \n",
    "æ®‹å·® + LayerNorm  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;â”‚  \n",
    "Feed-Forward Network  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;â”‚  \n",
    "æ®‹å·® + LayerNorm  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;â”‚  \n",
    "è¾“å‡º x (N, tgt_seq_len, embed_size)\n",
    "\n",
    "å¥½çš„ï¼Œæˆ‘ä»¬æ¥ä»”ç»†åˆ†æ Transformer Decoder ä¸­çš„ **`tgt_mask`** å’Œ **`src_mask`** çš„åŒºåˆ«å’Œä½œç”¨ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "### 1ï¸âƒ£ `tgt_mask`ï¼ˆTarget Mask / Decoder è‡ªæ³¨æ„åŠ›æ©ç ï¼‰\n",
    "\n",
    "**ç”¨é€”**ï¼š\n",
    "\n",
    "* ç”¨äº **Masked Self-Attention**ï¼Œå±è”½ Decoder å½“å‰é¢„æµ‹ä½ç½®èƒ½çœ‹åˆ°çš„æœªæ¥ tokenã€‚\n",
    "* ç›®çš„æ˜¯ **ä¿è¯è‡ªå›å½’ï¼ˆauto-regressiveï¼‰ç”Ÿæˆ** çš„æ­£ç¡®æ€§ï¼šæ¨¡å‹åªèƒ½çœ‹åˆ°å½“å‰ä½ç½®åŠä¹‹å‰çš„ tokenã€‚\n",
    "\n",
    "**å½¢å¼**ï¼š\n",
    "\n",
    "* é€šå¸¸æ˜¯ä¸€ä¸ª **ä¸‹ä¸‰è§’çŸ©é˜µ**ï¼ˆseq\\_len Ã— seq\\_lenï¼‰ï¼Œä¹Ÿå¯ä»¥å¹¿æ’­æˆ `(N, 1, seq_len, seq_len)`\n",
    "* å¯¹äºä½ç½® iï¼Œåªèƒ½æ³¨æ„åˆ° j â‰¤ i çš„ token\n",
    "* ç”¨åœ¨ `self_attention` ä¸­çš„ mask å‚æ•°\n",
    "\n",
    "**ç¤ºä¾‹ï¼ˆseq\\_len=4ï¼‰**ï¼š\n",
    "\n",
    "| i\\j | 0 | 1 | 2 | 3 |\n",
    "| --- | - | - | - | - |\n",
    "| 0   | 1 | 0 | 0 | 0 |\n",
    "| 1   | 1 | 1 | 0 | 0 |\n",
    "| 2   | 1 | 1 | 1 | 0 |\n",
    "| 3   | 1 | 1 | 1 | 1 |\n",
    "\n",
    "* 1 è¡¨ç¤ºå…è®¸æ³¨æ„\n",
    "* 0 è¡¨ç¤ºå±è”½æœªæ¥\n",
    "\n",
    "**ä½œç”¨**ï¼šç¡®ä¿ç”Ÿæˆæ—¶ **ä¸æ³„éœ²æœªæ¥ä¿¡æ¯**ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "### 2ï¸âƒ£ `src_mask`ï¼ˆSource Mask / Encoder-Decoder æ³¨æ„åŠ›æ©ç ï¼‰\n",
    "\n",
    "**ç”¨é€”**ï¼š\n",
    "\n",
    "* ç”¨äº **Cross-Attention**ï¼ˆDecoder æŸ¥è¯¢ Encoder è¾“å‡ºæ—¶ï¼‰\n",
    "* ç›®çš„æ˜¯ **å±è”½ Encoder è¾“å…¥ä¸­çš„ padding token**ï¼Œé¿å…æ¨¡å‹æŠŠæ— æ„ä¹‰çš„ padding å½“ä½œæœ‰ç”¨ä¿¡æ¯\n",
    "\n",
    "**å½¢å¼**ï¼š\n",
    "\n",
    "* å½¢çŠ¶ `(N, 1, 1, src_seq_len)` æˆ– `(N, 1, tgt_seq_len, src_seq_len)`\n",
    "* å¯¹åº” Encoder è¾“å…¥çš„æ¯ä¸ª tokenï¼Œ1 è¡¨ç¤ºå¯æ³¨æ„ï¼Œ0 è¡¨ç¤ºå±è”½\n",
    "\n",
    "**ç¤ºä¾‹ï¼ˆsrc\\_seq\\_len=5, padding åœ¨æœ€åä¸¤ä¸ªä½ç½®ï¼‰**ï¼š\n",
    "\n",
    "| token 0 | token 1 | token 2 | token 3 | token 4 |\n",
    "| ------- | ------- | ------- | ------- | ------- |\n",
    "| 1       | 1       | 1       | 0       | 0       |\n",
    "\n",
    "* Decoder åœ¨åš cross-attention æ—¶ï¼Œä¸ä¼šå…³æ³¨ padding\n",
    "\n",
    "**ä½œç”¨**ï¼šé¿å…æ¨¡å‹å­¦ä¹ æ— æ„ä¹‰çš„ padding ä¿¡æ¯ï¼Œæå‡è®­ç»ƒæ•ˆæœã€‚\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ”¹ æ€»ç»“å¯¹æ¯”\n",
    "\n",
    "| Mask ç±»å‹    | ä½¿ç”¨åœºæ™¯                              | å±è”½ç›®æ ‡                  | ä½œç”¨                   |\n",
    "| ---------- | --------------------------------- | --------------------- | -------------------- |\n",
    "| `tgt_mask` | Decoder è‡ªæ³¨æ„åŠ›                      | æœªæ¥ä½ç½®                  | ä¿è¯è‡ªå›å½’ç”Ÿæˆï¼Œåªä¾èµ–å·²ç”Ÿæˆ token |\n",
    "| `src_mask` | Decoder â†’ Encoder Cross-Attention | Encoder padding token | é¿å…å…³æ³¨æ— æ•ˆä¿¡æ¯ï¼Œæé«˜æ³¨æ„åŠ›æ•ˆç‡     |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05d8db18",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, embed_size, heads, qk_dim, ff_hidden_size, dropout=0.1):\n",
    "        super().__init__()\n",
    "        # 1ï¸âƒ£ Masked Self-Attentionï¼ˆDecoder å†…éƒ¨è‡ªæ³¨æ„åŠ›ï¼‰\n",
    "        self.self_attention = SelfAttention(embed_size, heads, qk_dim)\n",
    "        self.norm1 = nn.LayerNorm(embed_size)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "\n",
    "        # 2ï¸âƒ£ Cross-Attentionï¼ˆDecoder æŸ¥è¯¢ Encoder è¾“å‡ºï¼‰\n",
    "        self.cross_attention = SelfAttention(embed_size, heads, qk_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_size)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "        # 3ï¸âƒ£ Feed-Forward Network\n",
    "        self.feed_forward = FeedForward(embed_size, ff_hidden_size, dropout)\n",
    "        self.norm3 = nn.LayerNorm(embed_size)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, enc_out, src_mask=None, tgt_mask=None):\n",
    "        \"\"\"\n",
    "        x:        (N, tgt_seq_len, embed_size)  Decoder è¾“å…¥\n",
    "        enc_out:  (N, src_seq_len, embed_size)  Encoder è¾“å‡º\n",
    "        src_mask: (N, 1, 1, src_seq_len)        Encoder-Decoder æ³¨æ„åŠ› mask\n",
    "        tgt_mask: (N, 1, tgt_seq_len, tgt_seq_len)  Decoder è‡ªæ³¨æ„åŠ› mask\n",
    "        \"\"\"\n",
    "\n",
    "        # -----------------------------\n",
    "        # 1ï¸âƒ£ Masked Self-Attention\n",
    "        # Q/K/V éƒ½æ¥è‡ª x\n",
    "        # tgt_mask ç”¨äºå±è”½æœªæ¥ä¿¡æ¯ï¼ˆä¸‹ä¸‰è§’ maskï¼‰\n",
    "        # è¾“å‡ºç»´åº¦: (N, tgt_seq_len, embed_size)\n",
    "        # -----------------------------\n",
    "        _x = self.self_attention(query=x, key=None, value=None, mask=tgt_mask)\n",
    "        x = self.norm1(x + self.dropout1(_x))  # æ®‹å·®è¿æ¥ + LayerNorm\n",
    "\n",
    "        # -----------------------------\n",
    "        # 2ï¸âƒ£ Cross-Attention\n",
    "        # Q æ¥è‡ª Decoder å½“å‰è¾“å…¥ x\n",
    "        # K/V æ¥è‡ª Encoder è¾“å‡º enc_out\n",
    "        # src_mask ç”¨äºå±è”½ padding token\n",
    "        # è¾“å‡ºç»´åº¦: (N, tgt_seq_len, embed_size)\n",
    "        # -----------------------------\n",
    "        _x = self.cross_attention(query=x, key=enc_out, value=enc_out, mask=src_mask)\n",
    "        x = self.norm2(x + self.dropout2(_x))  # æ®‹å·®è¿æ¥ + LayerNorm\n",
    "\n",
    "        # -----------------------------\n",
    "        # 3ï¸âƒ£ Feed-Forward Network\n",
    "        # è¾“å‡ºç»´åº¦: (N, tgt_seq_len, embed_size)\n",
    "        # -----------------------------\n",
    "        _x = self.feed_forward(x)\n",
    "        x = self.norm3(x + self.dropout3(_x))  # æ®‹å·®è¿æ¥ + LayerNorm\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a63a84",
   "metadata": {},
   "source": [
    "## Transformer Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62e04838",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, num_layers, heads, qk_dim,\n",
    "                 ff_hidden_size, dropout=0.1, max_length=100):\n",
    "        super().__init__()\n",
    "        self.embed_size = embed_size\n",
    "\n",
    "        # 1ï¸âƒ£ è¯åµŒå…¥å±‚ï¼Œå°† token id è½¬æ¢ä¸ºå‘é‡\n",
    "        self.word_embedding = nn.Embedding(vocab_size, embed_size)\n",
    "\n",
    "        # 2ï¸âƒ£ ä½ç½®ç¼–ç ï¼ŒåŠ å…¥ä½ç½®ä¿¡æ¯\n",
    "        self.position_encoding = PositionalEncoding(embed_size, max_length)\n",
    "\n",
    "        # 3ï¸âƒ£ å¤šå±‚ DecoderBlock\n",
    "        self.layers = nn.ModuleList([\n",
    "            DecoderBlock(embed_size, heads, qk_dim, ff_hidden_size, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        # 4ï¸âƒ£ è¾“å…¥ dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # 5ï¸âƒ£ è¾“å‡ºæŠ•å½±å±‚ï¼Œå°† embed_size æ˜ å°„åˆ° vocab_size\n",
    "        self.fc_out = nn.Linear(embed_size, vocab_size)\n",
    "\n",
    "    def forward(self, x, enc_out, src_mask=None, tgt_mask=None):\n",
    "        \"\"\"\n",
    "        x: (N, tgt_seq_len) ç›®æ ‡åºåˆ— token id\n",
    "        enc_out: (N, src_seq_len, embed_size) Encoder è¾“å‡º\n",
    "        src_mask: (N, 1, 1, src_seq_len) Encoder padding mask\n",
    "        tgt_mask: (N, 1, tgt_seq_len, tgt_seq_len) Decoder è‡ªæ³¨æ„åŠ› mask\n",
    "        \"\"\"\n",
    "\n",
    "        # 1ï¸âƒ£ token embedding + positional encoding\n",
    "        out = self.word_embedding(x)            # (N, tgt_seq_len, embed_size)\n",
    "        out = self.position_encoding(out)       # åŠ å…¥ä½ç½®ä¿¡æ¯\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        # 2ï¸âƒ£ å¤šå±‚ DecoderBlock å †å \n",
    "        for layer in self.layers:\n",
    "            out = layer(out, enc_out, src_mask, tgt_mask)\n",
    "            # out ç»´åº¦ä¿æŒ (N, tgt_seq_len, embed_size)\n",
    "\n",
    "        # 3ï¸âƒ£ è¾“å‡ºæŠ•å½±åˆ° vocab\n",
    "        out = self.fc_out(out)                  # (N, tgt_seq_len, vocab_size)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5b9862",
   "metadata": {},
   "source": [
    "æœ‰åŒºåˆ«çš„ï¼Œè™½ç„¶åœ¨å½¢å¼ä¸Šéƒ½æ˜¯ `(N, seq_len)` çš„ token idï¼Œä½† **å«ä¹‰å’Œä½œç”¨ä¸åŒ**ï¼š\n",
    "\n",
    "---\n",
    "\n",
    "### 1ï¸âƒ£ Encoder çš„è¾“å…¥ `x`\n",
    "\n",
    "* **ç±»å‹**ï¼šæºåºåˆ—ï¼ˆsource sequenceï¼‰çš„ token id\n",
    "* **ä½œç”¨**ï¼šç¼–ç æºåºåˆ—ä¿¡æ¯ï¼Œç”Ÿæˆä¸Šä¸‹æ–‡å‘é‡ `enc_out`\n",
    "* **è¾“å…¥ç¤ºä¾‹**ï¼šä¸€æ®µè‹±æ–‡å¥å­ `[The, cat, sits, .]` â†’ `[101, 205, 330, 102]`\n",
    "* **ç‰¹ç‚¹**ï¼š\n",
    "\n",
    "  * æ‰€æœ‰ token éƒ½å¯ç”¨\n",
    "  * Encoder é€šå¸¸ä¸å±è”½æœªæ¥ token\n",
    "  * å¯åŠ  `src_mask` å±è”½ padding\n",
    "\n",
    "---\n",
    "\n",
    "### 2ï¸âƒ£ Decoder çš„è¾“å…¥ `x`\n",
    "\n",
    "* **ç±»å‹**ï¼šç›®æ ‡åºåˆ—ï¼ˆtarget sequenceï¼‰çš„ token id\n",
    "* **ä½œç”¨**ï¼š\n",
    "\n",
    "  * ç”¨äº **è®­ç»ƒé˜¶æ®µ**ï¼šåšè‡ªå›å½’é¢„æµ‹ï¼Œç”Ÿæˆä¸‹ä¸€ä¸ª token\n",
    "  * ç”¨äº **æ¨ç†é˜¶æ®µ**ï¼šä» `<SOS>` å¼€å§‹ï¼Œé€æ­¥ç”Ÿæˆ token\n",
    "* **è¾“å…¥ç¤ºä¾‹**ï¼šç¿»è¯‘ä»»åŠ¡ä¸­ç›®æ ‡å¥å­ `[Le, chat, est, assis]` â†’ `[101, 400, 500, 102]`\n",
    "* **ç‰¹ç‚¹**ï¼š\n",
    "\n",
    "  * éœ€è¦ **Masked Self-Attention** (`tgt_mask`) é¿å…çœ‹åˆ°æœªæ¥ token\n",
    "  * Cross-Attention æŸ¥è¯¢ Encoder è¾“å‡ºï¼ˆ`enc_out`ï¼‰\n",
    "\n",
    "---\n",
    "\n",
    "#### ğŸ”¹ æ€»ç»“\n",
    "\n",
    "| æ¨¡å—      | è¾“å…¥ `x` ç±»å‹                        | ä½œç”¨                               |\n",
    "| ------- | -------------------------------- | -------------------------------- |\n",
    "| Encoder | æºåºåˆ— token id `(N, src_seq_len)`  | ç¼–ç æºåºåˆ—ä¸Šä¸‹æ–‡ï¼Œç”Ÿæˆ `enc_out`            |\n",
    "| Decoder | ç›®æ ‡åºåˆ— token id `(N, tgt_seq_len)` | è‡ªå›å½’é¢„æµ‹ä¸‹ä¸€ tokenï¼Œç»“åˆ Encoder ä¸Šä¸‹æ–‡ç”Ÿæˆè¾“å‡º |\n",
    "\n",
    "---\n",
    "\n",
    "å¦‚æœä½ æ„¿æ„ï¼Œæˆ‘å¯ä»¥ç”»ä¸€å¼  **Encoder å’Œ Decoder è¾“å…¥è¾“å‡ºç¤ºæ„å›¾**ï¼Œç›´è§‚æ˜¾ç¤º `x`ã€`enc_out`ã€`tgt_mask`ã€`src_mask` ä¹‹é—´çš„å…³ç³»ï¼Œè¿™æ ·ç†è§£ä¼šæ›´æ¸…æ¥šã€‚\n",
    "\n",
    "ä½ å¸Œæœ›æˆ‘ç”»å—ï¼Ÿ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd76b97f",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97fd9da0",
   "metadata": {},
   "source": [
    "src â”€â”€â–º Encoder â”€â”€â–º enc_out  \n",
    "tgt â”€â”€â–º Decoder(x = tgt, enc_out, src_mask, tgt_mask) â”€â”€â–º out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "174feabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, embed_size=256, num_layers=3,\n",
    "                 heads=8, qk_dim=64, ff_hidden_size=512, dropout=0.1, max_length=100):\n",
    "        \"\"\"\n",
    "        src_vocab_size: æºè¯­è¨€è¯è¡¨å¤§å°\n",
    "        tgt_vocab_size: ç›®æ ‡è¯­è¨€è¯è¡¨å¤§å°\n",
    "        embed_size: Embedding/æ¨¡å‹ç»´åº¦ (d_model)\n",
    "        num_layers: Encoder/Decoder å±‚æ•°\n",
    "        heads: æ³¨æ„åŠ›å¤´æ•°\n",
    "        qk_dim: Q/K æŠ•å½±ç»´åº¦\n",
    "        ff_hidden_size: å‰é¦ˆç½‘ç»œéšè—å±‚å¤§å°\n",
    "        dropout: Dropout æ¦‚ç‡\n",
    "        max_length: åºåˆ—æœ€å¤§é•¿åº¦\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # 1ï¸âƒ£ Encoder\n",
    "        self.encoder = TransformerEncoder(\n",
    "            vocab_size=src_vocab_size,\n",
    "            embed_size=embed_size,\n",
    "            num_layers=num_layers,\n",
    "            heads=heads,\n",
    "            qk_dim=qk_dim,\n",
    "            ff_hidden_size=ff_hidden_size,\n",
    "            dropout=dropout,\n",
    "            max_length=max_length\n",
    "        )\n",
    "\n",
    "        # 2ï¸âƒ£ Decoder\n",
    "        self.decoder = TransformerDecoder(\n",
    "            vocab_size=tgt_vocab_size,\n",
    "            embed_size=embed_size,\n",
    "            num_layers=num_layers,\n",
    "            heads=heads,\n",
    "            qk_dim=qk_dim,\n",
    "            ff_hidden_size=ff_hidden_size,\n",
    "            dropout=dropout,\n",
    "            max_length=max_length\n",
    "        )\n",
    "\n",
    "    # -----------------------------\n",
    "    # æ„é€ æºåºåˆ— mask\n",
    "    # -----------------------------\n",
    "    def make_src_mask(self, src):\n",
    "        \"\"\"\n",
    "        src: (N, src_seq_len)\n",
    "        src_mask: (N, 1, 1, src_seq_len)\n",
    "        mask ä¸º 1 çš„ä½ç½®è¡¨ç¤ºæœ‰æ•ˆ tokenï¼Œä¸º 0 çš„ä½ç½®è¡¨ç¤º padding\n",
    "        \"\"\"\n",
    "        return (src != 0).unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "    # -----------------------------\n",
    "    # æ„é€ ç›®æ ‡åºåˆ— mask\n",
    "    # -----------------------------\n",
    "    def make_tgt_mask(self, tgt):\n",
    "        \"\"\"\n",
    "        tgt: (N, tgt_seq_len)\n",
    "        tgt_mask: (N, 1, tgt_seq_len, tgt_seq_len)\n",
    "        mask ä¸ºä¸‹ä¸‰è§’çŸ©é˜µï¼Œé˜²æ­¢ Decoder çœ‹åˆ°æœªæ¥ token\n",
    "        \"\"\"\n",
    "        N, tgt_len = tgt.shape\n",
    "        # ä¸‹ä¸‰è§’çŸ©é˜µ\n",
    "        tgt_mask = torch.tril(torch.ones((tgt_len, tgt_len))).expand(N, 1, tgt_len, tgt_len)\n",
    "        return tgt_mask.to(tgt.device)\n",
    "\n",
    "    # -----------------------------\n",
    "    # å‰å‘ä¼ æ’­\n",
    "    # -----------------------------\n",
    "    def forward(self, src, tgt):\n",
    "        \"\"\"\n",
    "        src: (N, src_seq_len) æºåºåˆ— token id\n",
    "        tgt: (N, tgt_seq_len) ç›®æ ‡åºåˆ— token id\n",
    "        \"\"\"\n",
    "        # 1ï¸âƒ£ æ„é€  mask\n",
    "        src_mask = self.make_src_mask(src)\n",
    "        tgt_mask = self.make_tgt_mask(tgt)\n",
    "\n",
    "        # 2ï¸âƒ£ Encoder ç¼–ç æºåºåˆ—\n",
    "        enc_out = self.encoder(src, mask=src_mask)  # (N, src_seq_len, embed_size)\n",
    "\n",
    "        # 3ï¸âƒ£ Decoder è§£ç ç›®æ ‡åºåˆ—\n",
    "        out = self.decoder(tgt, enc_out, src_mask=src_mask, tgt_mask=tgt_mask)\n",
    "        # è¾“å‡ºç»´åº¦: (N, tgt_seq_len, tgt_vocab_size)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "318bfd4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src shape: torch.Size([2, 6])\n",
      "tgt shape: torch.Size([2, 6])\n",
      "æ¨¡å‹è¾“å‡º shape: torch.Size([2, 6, 1000])\n",
      "è¾“å‡ºç¤ºä¾‹ï¼ˆéƒ¨åˆ†ï¼‰ï¼š [-0.2812156   0.4754691   0.96160126  0.35761005  0.6679771 ]\n"
     ]
    }
   ],
   "source": [
    "# ç¤ºä¾‹ï¼šæ„é€ è¾“å…¥å¹¶è¿è¡Œ Transformer\n",
    "\n",
    "import torch\n",
    "\n",
    "# å‡è®¾è¯è¡¨å¤§å°ä¸º 1000ï¼Œæºåºåˆ—å’Œç›®æ ‡åºåˆ—é•¿åº¦å‡ä¸º 6\n",
    "src_vocab_size = 1000\n",
    "tgt_vocab_size = 1000\n",
    "src_seq_len = 6\n",
    "tgt_seq_len = 6\n",
    "batch_size = 2\n",
    "\n",
    "# æ„é€ éšæœº token idï¼ˆæ¨¡æ‹Ÿè¾“å…¥ï¼‰\n",
    "src = torch.randint(1, src_vocab_size, (batch_size, src_seq_len))\n",
    "tgt = torch.randint(1, tgt_vocab_size, (batch_size, tgt_seq_len))\n",
    "\n",
    "# å®ä¾‹åŒ–æ¨¡å‹\n",
    "model = Transformer(\n",
    "    src_vocab_size=src_vocab_size,\n",
    "    tgt_vocab_size=tgt_vocab_size,\n",
    "    embed_size=32,        # å¯ä»¥è°ƒå°ä»¥ä¾¿å¿«é€Ÿå®éªŒ\n",
    "    num_layers=2,\n",
    "    heads=4,\n",
    "    qk_dim=32,\n",
    "    ff_hidden_size=64,\n",
    "    dropout=0.1,\n",
    "    max_length=20\n",
    ")\n",
    "\n",
    "# å‰å‘æ¨ç†\n",
    "out = model(src, tgt)\n",
    "\n",
    "print(\"src shape:\", src.shape)\n",
    "print(\"tgt shape:\", tgt.shape)\n",
    "print(\"æ¨¡å‹è¾“å‡º shape:\", out.shape)  # (batch_size, tgt_seq_len, tgt_vocab_size)\n",
    "print(\"è¾“å‡ºç¤ºä¾‹ï¼ˆéƒ¨åˆ†ï¼‰ï¼š\", out[0, 0, :5].detach().numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
