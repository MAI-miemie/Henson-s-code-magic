{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c7e1a11",
   "metadata": {},
   "source": [
    "## Transformer Encoder 的每个 Block 结构详解\n",
    "\n",
    "1. **多头自注意力机制（Multi-Head Self-Attention）**\n",
    "   - 输入序列首先通过线性变换，分别生成 Query（查询）、Key（键）、Value（值）三个矩阵。\n",
    "   - 每个位置的 Query 与所有位置的 Key 计算相关性分数（注意力分数），并对 Value 加权求和，得到新的表示。\n",
    "   - 多头机制将输入拆分为多个子空间，分别计算注意力，最后拼接结果，增强模型表达能力。\n",
    "\n",
    "2. **残差连接与 LayerNorm**\n",
    "   - 自注意力输出与原输入做残差连接（相加），再进行 Layer Normalization（层归一化），提升训练稳定性，加速收敛。\n",
    "\n",
    "3. **前馈全连接网络（Feed Forward Network, FFN）**\n",
    "   - 每个位置的输出分别通过两层线性变换和激活函数（如 ReLU 或 GELU），结构通常为：`Linear -> Activation -> Linear`。\n",
    "   - 进一步提升每个位置的特征表达能力。\n",
    "\n",
    "4. **再次残差连接与 LayerNorm**\n",
    "   - FFN 输出与自注意力后的结果做残差连接，再进行 Layer Normalization，最终输出每个 block 的结果。\n",
    "\n",
    "---\n",
    "\n",
    "**整体结构流程图：**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b11e76",
   "metadata": {},
   "source": [
    "输入  \n",
    "↓  \n",
    "位置编码(Positional Encoding)  \n",
    "↓   \n",
    "多头自注意力（Multi-Head Self-Attention）  \n",
    "↓  \n",
    "残差连接 + LayerNorm  \n",
    "↓  \n",
    "前馈网络（Feed Forward Network）  \n",
    "↓  \n",
    "残差连接 + LayerNorm  \n",
    "↓  \n",
    "输出"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76bcfb0",
   "metadata": {},
   "source": [
    "## 位置编码Positional_Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2646a76f",
   "metadata": {},
   "source": [
    "你这段 `PositionalEncoding` 是 Transformer **Encoder** 里常用的**位置编码（Positional Encoding）**实现，核心作用是：\n",
    "给纯粹的词向量（word embedding）加入**位置信息**，让模型能区分“我爱你”和“你爱我”这种单词顺序不同的句子。\n",
    "\n",
    "下面我按结构拆开讲：\n",
    "\n",
    "---\n",
    "\n",
    "### 1. **为什么需要位置编码？**\n",
    "\n",
    "Transformer 的 **Self-Attention** 机制本身是**顺序无关**的，它只看 token 间的相似度，不知道谁在前谁在后。\n",
    "所以必须给每个 token 加一个**位置向量**（position vector），这样模型才知道 token 在句子里的位置。\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **代码执行流程**\n",
    "\n",
    "#### (1) 创建空矩阵\n",
    "\n",
    "```python\n",
    "pe = torch.zeros(max_len, embed_size)\n",
    "```\n",
    "\n",
    "* `max_len`：句子最大长度（5000个位置）。\n",
    "* `embed_size`：词向量维度。\n",
    "* 每行代表一个位置（pos），每列代表该位置对应的一个维度。\n",
    "\n",
    "---\n",
    "\n",
    "#### (2) 生成位置索引\n",
    "\n",
    "```python\n",
    "position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "```\n",
    "\n",
    "* 得到 `[0, 1, 2, ..., max_len-1]` 的列向量，表示位置 ID。\n",
    "* `unsqueeze(1)` 把它变成 `(max_len, 1)` 方便广播计算。\n",
    "\n",
    "---\n",
    "\n",
    "#### (3) 生成频率缩放因子\n",
    "\n",
    "```python\n",
    "div_term = torch.exp(\n",
    "    torch.arange(0, embed_size, 2).float() * (-math.log(10000.0) / embed_size)\n",
    ")\n",
    "```\n",
    "\n",
    "* `torch.arange(0, embed_size, 2)`：取偶数下标（0, 2, 4...）。\n",
    "* 缩放因子公式：\n",
    "\n",
    "  $$\n",
    "  \\frac{1}{10000^{2i / d_{\\text{model}}}}\n",
    "  $$\n",
    "\n",
    "  这是论文 *Attention is All You Need* 提出的，让不同维度的波形频率不同，便于模型捕捉短距离和长距离的依赖。\n",
    "\n",
    "---\n",
    "\n",
    "#### (4) 计算 sin/cos 位置编码\n",
    "\n",
    "```python\n",
    "pe[:, 0::2] = torch.sin(position * div_term)  # 偶数列\n",
    "pe[:, 1::2] = torch.cos(position * div_term)  # 奇数列\n",
    "```\n",
    "\n",
    "* **偶数维**：`sin(pos / 10000^(2i/d))`\n",
    "* **奇数维**：`cos(pos / 10000^(2i/d))`\n",
    "* 为什么用 sin/cos？\n",
    "\n",
    "  * 周期性 → 可以推广到比训练时更长的序列。\n",
    "  * 不同维度有不同周期 → 模型可线性组合得到任何相对位置。\n",
    "\n",
    "---\n",
    "\n",
    "#### (5) 形状调整\n",
    "\n",
    "```python\n",
    "pe = pe.unsqueeze(0)  # (1, max_len, embed_size)\n",
    "```\n",
    "\n",
    "* 多加一个 batch 维度，方便和 `(batch_size, seq_len, embed_size)` 的输入对齐。\n",
    "\n",
    "---\n",
    "\n",
    "#### (6) 注册 buffer\n",
    "\n",
    "```python\n",
    "self.register_buffer('pe', pe)\n",
    "```\n",
    "\n",
    "* `pe` 不是参数（不会被梯度更新），但会随模型保存/加载。\n",
    "* 避免被误认为是可训练权重。\n",
    "\n",
    "---\n",
    "\n",
    "#### (7) 前向传播\n",
    "\n",
    "```python\n",
    "x = x + self.pe[:, :x.size(1), :]\n",
    "```\n",
    "\n",
    "* 取出和当前句子长度 `seq_len` 匹配的那一段位置编码。\n",
    "* 直接 **逐元素相加** 到词向量 embedding 上，得到带位置信息的输入。\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **整体过程总结**\n",
    "\n",
    "**输入**：词向量矩阵 `(batch_size, seq_len, embed_size)`\n",
    "**输出**：加入位置编码的词向量 `(batch_size, seq_len, embed_size)`\n",
    "\n",
    "> 原理上，Encoder 在送进多头注意力之前，先用：\n",
    ">\n",
    "> $$\n",
    "> x_{\\text{pos}} = x_{\\text{embed}} + PE\n",
    "> $$\n",
    ">\n",
    "> 这样 Attention 就能利用相对位置信息。\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b67c8ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "# 位置编码\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embed_size, max_len=5000):\n",
    "        super().__init__()\n",
    "        # 创建一个全零矩阵，形状为 (max_len, embed_size)，用于存储每个位置的编码\n",
    "        pe = torch.zeros(max_len, embed_size)\n",
    "        # 生成位置索引，形状为 (max_len, 1)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        # 计算每个维度的缩放因子，公式来自论文\n",
    "        div_term = torch.exp(torch.arange(0, embed_size, 2).float() * (-math.log(10000.0) / embed_size))\n",
    "        # 偶数维度用 sin，奇数维度用 cos，生成不同频率的波形\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        # 增加 batch 维度，变成 (1, max_len, embed_size)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        # 注册为 buffer，不参与训练，但会随模型保存\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, seq_len, embed_size)\n",
    "        # 取出和输入序列长度匹配的那一段位置编码，并加到输入上\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eaa6fa5",
   "metadata": {},
   "source": [
    "## 多头自注意力机制"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675fff65",
   "metadata": {},
   "source": [
    "(N, seq_len, embed_size)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;↓ Q/K 投影到 qk_dim，V 保持 embed_size  \n",
    "(N, seq_len, qk_dim) / (N, seq_len, embed_size)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;↓ 多头拆分  \n",
    "(N, heads, seq_len, head_dim_qk) / (N, heads, seq_len, head_dim_v)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;↓ 注意力计算  \n",
    "(N, heads, seq_len, head_dim_v)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;↓ 多头合并  \n",
    "(N, seq_len, embed_size)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;↓ 输出线性层  \n",
    "(N, seq_len, embed_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5683d000",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 多头自注意力机制\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, embed_size, heads, qk_dim):\n",
    "        \"\"\"\n",
    "        embed_size: 输入 token 向量维度（d_model）\n",
    "        heads: 注意力头数\n",
    "        qk_dim: Q/K 投影后的维度（会均分到每个头）\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.heads = heads\n",
    "        self.head_dim_qk = qk_dim // heads           # 每个头的 Q/K 维度\n",
    "        self.head_dim_v = embed_size // heads        # 每个头的 V 维度（保持和输入一致）\n",
    "\n",
    "        assert qk_dim % heads == 0, \"qk_dim 必须能被 heads 整除\"\n",
    "        assert embed_size % heads == 0, \"embed_size 必须能被 heads 整除\"\n",
    "\n",
    "        # Q/K 的线性映射到 qk_dim\n",
    "        self.query = nn.Linear(embed_size, qk_dim)\n",
    "        self.key = nn.Linear(embed_size, qk_dim)\n",
    "        # V 保持 embed_size 维\n",
    "        self.value = nn.Linear(embed_size, embed_size)\n",
    "\n",
    "        # 输出线性层\n",
    "        self.fc_out = nn.Linear(embed_size, embed_size)\n",
    "\n",
    "    def forward(self, query, key=None, value=None, mask=None):\n",
    "        \"\"\"\n",
    "        query: (N, seq_len_q, embed_size)\n",
    "        key:   (N, seq_len_k, embed_size)，默认 None，则用 query\n",
    "        value: (N, seq_len_v, embed_size)，默认 None，则用 query\n",
    "        mask:  注意力 mask，可选\n",
    "        \"\"\"\n",
    "        if key is None:\n",
    "            key = query\n",
    "        if value is None:\n",
    "            value = query\n",
    "\n",
    "        N = query.shape[0]\n",
    "        query_len = query.shape[1]\n",
    "        key_len = key.shape[1]\n",
    "        value_len = value.shape[1]\n",
    "\n",
    "        # 1️⃣ Q/K/V 投影\n",
    "        Q = self.query(query)    # (N, query_len, qk_dim)\n",
    "        K = self.key(key)        # (N, key_len, qk_dim)\n",
    "        V = self.value(value)    # (N, value_len, embed_size)\n",
    "\n",
    "        # 2️⃣ 拆成多头\n",
    "        Q = Q.view(N, query_len, self.heads, self.head_dim_qk).transpose(1, 2)  # (N, heads, query_len, head_dim_qk)\n",
    "        K = K.view(N, key_len, self.heads, self.head_dim_qk).transpose(1, 2)    # (N, heads, key_len, head_dim_qk)\n",
    "        V = V.view(N, value_len, self.heads, self.head_dim_v).transpose(1, 2)   # (N, heads, value_len, head_dim_v)\n",
    "\n",
    "        # 3️⃣ Scaled Dot-Product Attention\n",
    "        energy = torch.matmul(Q, K.transpose(-1, -2)) / math.sqrt(self.head_dim_qk)\n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask == 0, float(\"-1e20\"))\n",
    "\n",
    "        attention = torch.softmax(energy, dim=-1)  # (N, heads, query_len, key_len)\n",
    "\n",
    "        # 4️⃣ 注意力加权 V\n",
    "        out = torch.matmul(attention, V)  # (N, heads, query_len, head_dim_v)\n",
    "\n",
    "        # 5️⃣ 多头拼接\n",
    "        out = out.transpose(1, 2).contiguous().view(N, query_len, self.embed_size)  # (N, query_len, embed_size)\n",
    "\n",
    "        # 6️⃣ 输出线性层\n",
    "        out = self.fc_out(out)  # (N, query_len, embed_size)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9124c907",
   "metadata": {},
   "source": [
    "## 前馈神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45285246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 前馈神经网络\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(embed_size, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, embed_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a501a2",
   "metadata": {},
   "source": [
    "## 每个Endoder Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "661681ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder Block\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, embed_size, heads, qk_dim, ffn_hidden, dropout=0.1):\n",
    "        super().__init__()\n",
    "        # 使用通用 SelfAttention\n",
    "        self.attention = SelfAttention(embed_size, heads, qk_dim)\n",
    "        self.norm1 = nn.LayerNorm(embed_size)\n",
    "        self.norm2 = nn.LayerNorm(embed_size)\n",
    "        self.ffn = FeedForward(embed_size, ffn_hidden, dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # 1️⃣ 自注意力 + 残差 + LayerNorm\n",
    "        attn_out = self.attention(query=x, mask=mask)  # Q/K/V 都是 x\n",
    "        x = self.norm1(x + self.dropout(attn_out))\n",
    "\n",
    "        # 2️⃣ 前馈网络 + 残差 + LayerNorm\n",
    "        ffn_out = self.ffn(x)\n",
    "        x = self.norm2(x + self.dropout(ffn_out))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b55984f",
   "metadata": {},
   "source": [
    "## Transformer Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8601a67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer Encoder\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, num_layers, heads, qk_dim,\n",
    "                 ff_hidden_size, dropout=0.1, max_length=100):\n",
    "        super().__init__()\n",
    "        self.embed_size = embed_size\n",
    "        \n",
    "        # 1️⃣ 词嵌入\n",
    "        self.word_embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        \n",
    "        # 2️⃣ 位置编码\n",
    "        self.position_encoding = PositionalEncoding(embed_size, max_length)\n",
    "        \n",
    "        # 3️⃣ 多层 EncoderBlock\n",
    "        self.layers = nn.ModuleList([\n",
    "            EncoderBlock(embed_size, heads, qk_dim, ff_hidden_size, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # 4️⃣ 输入 dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # x: (N, seq_len)\n",
    "        out = self.word_embedding(x)           # (N, seq_len, embed_size)\n",
    "        out = self.position_encoding(out)      # 加入位置信息\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        # 堆叠多层 EncoderBlock\n",
    "        for layer in self.layers:\n",
    "            out = layer(out, mask)             # (N, seq_len, embed_size)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498191d3",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "`vocab_size` 和 `max_length` 看起来都是整数参数，但它们代表的意义完全不同：\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 `vocab_size`\n",
    "\n",
    "* **定义**：词表大小，即模型能识别的 **不同 token（词/子词/字符）数量**。\n",
    "* **作用**：作为 `nn.Embedding(vocab_size, embed_size)` 的输入维度。\n",
    "\n",
    "  * 每个 token ID 是 `[0, vocab_size-1]` 的整数索引\n",
    "  * Embedding 层会把 ID 映射为一个 `embed_size` 维的向量\n",
    "\n",
    "👉 举例：\n",
    "\n",
    "* 如果 `vocab_size = 30,000`，说明模型词表里有 30k 个不同的 token。\n",
    "* 输入 token ID `42`，Embedding 会查表得到 `word_embedding[42]` 这个向量。\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 `max_length`\n",
    "\n",
    "* **定义**：模型支持的 **序列最大长度**，通常用于 **位置编码 (Positional Encoding)**。\n",
    "* **作用**：告诉模型“我最多处理多长的输入序列”。\n",
    "\n",
    "  * 在 Transformer 里，位置编码是一个 `max_length × embed_size` 的矩阵。\n",
    "  * 输入序列长度不能超过 `max_length`，否则就没有对应的位置编码。\n",
    "\n",
    "👉 举例：\n",
    "\n",
    "* 如果 `max_length = 100`，表示输入/输出序列最长不能超过 100 个 token。\n",
    "* 当输入句子长 80 时，只用前 80 个位置编码；当输入句子长 120 时，就会报错（超出最大长度）。\n",
    "\n",
    "---\n",
    "\n",
    "### ⚖️ 区别总结\n",
    "\n",
    "| 参数               | 控制什么                  | 决定的限制                |\n",
    "| ---------------- | --------------------- | -------------------- |\n",
    "| **`vocab_size`** | 词表的广度（能识别多少个不同 token） | 不能输入超出词表范围的 token id |\n",
    "| **`max_length`** | 序列的深度（一次能处理多少个 token） | 不能输入超过该长度的序列         |\n",
    "\n",
    "---\n",
    "\n",
    "#### 类比理解 🎯\n",
    "\n",
    "* `vocab_size` 像 **字典里有多少个词**\n",
    "* `max_length` 像 **一句话最多能有多少个词**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d300fded",
   "metadata": {},
   "source": [
    "好的，我们来仔细拆解这一行代码：\n",
    "\n",
    "```python\n",
    "self.word_embedding = nn.Embedding(vocab_size, embed_size)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 1. `nn.Embedding` 的作用\n",
    "\n",
    "* 它的本质是一个 **查表层 (lookup table)**。\n",
    "* 输入：整数索引 (token id)，范围是 `[0, vocab_size-1]`。\n",
    "* 输出：对应的 **稠密向量** (embedding)，维度为 `embed_size`。\n",
    "\n",
    "也就是说：\n",
    "\n",
    "* 模型不会直接处理离散的 token id，而是先把它转成连续的向量。\n",
    "* 这个向量能捕捉到 token 的语义信息（在训练中学习到）。\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 2. 输入和输出形状\n",
    "\n",
    "假设定义：\n",
    "\n",
    "```python\n",
    "word_embedding = nn.Embedding(vocab_size=10000, embed_size=256)\n",
    "```\n",
    "\n",
    "#### 输入 (token id 序列)\n",
    "\n",
    "* 输入通常是一个 `LongTensor`，形状：\n",
    "\n",
    "  ```\n",
    "  (N, seq_len)\n",
    "  ```\n",
    "\n",
    "  * `N`: batch size\n",
    "  * `seq_len`: 序列长度\n",
    "\n",
    "举例：\n",
    "\n",
    "```python\n",
    "x = torch.tensor([[2, 45, 78], [5, 9, 100]])  # (N=2, seq_len=3)\n",
    "```\n",
    "\n",
    "表示：\n",
    "\n",
    "* 第一个样本序列 = \\[2, 45, 78]\n",
    "* 第二个样本序列 = \\[5, 9, 100]\n",
    "\n",
    "---\n",
    "\n",
    "#### 输出 (embedding 向量序列)\n",
    "\n",
    "* 输出的形状：\n",
    "\n",
    "  ```\n",
    "  (N, seq_len, embed_size)\n",
    "  ```\n",
    "\n",
    "  * 每个 token id 被映射成一个 `embed_size` 维的向量\n",
    "\n",
    "举例：\n",
    "\n",
    "```python\n",
    "out = word_embedding(x)  # (2, 3, 256)\n",
    "```\n",
    "\n",
    "输出：\n",
    "\n",
    "* batch 里有 2 个句子\n",
    "* 每个句子长度 = 3\n",
    "* 每个 token 变成 256 维向量\n",
    "\n",
    "所以最终 `out` 是：\n",
    "\n",
    "```\n",
    "[\n",
    "  [ [e_2], [e_45], [e_78]   ],   # 第一个句子的 embedding 序列\n",
    "  [ [e_5], [e_9],  [e_100] ]    # 第二个句子的 embedding 序列\n",
    "]\n",
    "```\n",
    "\n",
    "其中 `[e_x]` 表示 token x 的 embedding 向量，维度是 `256`。\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 3. 与 Transformer 的关系\n",
    "\n",
    "* 这是 **Encoder/Decoder 的第一步**。\n",
    "* 输入 token id → word embedding → 加上 position encoding → 送进后续的 self-attention。\n",
    "\n",
    "---\n",
    "\n",
    "⚡ 总结一句：\n",
    "`nn.Embedding(vocab_size, embed_size)` 就是把 **离散的单词 id (N, seq\\_len)** 转成 **稠密的语义向量序列 (N, seq\\_len, embed\\_size)**，为后续 Transformer 提供连续空间的输入。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037286ab",
   "metadata": {},
   "source": [
    "| 参数               | 含义                                                    |\n",
    "| ---------------- | ----------------------------------------------------- |\n",
    "| `vocab_size`     | 词表大小，用于 nn.Embedding，把 token id 映射成向量                 |\n",
    "| `embed_size`     | embedding 维度，也就是 d\\_model，所有 Encoder 层输入输出维度一致        |\n",
    "| `num_layers`     | Encoder 堆叠的层数，也就是有多少个 EncoderBlock                    |\n",
    "| `heads`          | 多头注意力头数，注意力机制会把 Q/K/V 拆成多头                            |\n",
    "| `qk_dim`         | Q/K 投影后的维度（可以小于 embed\\_size），每个头的维度 = qk\\_dim / heads |\n",
    "| `ff_hidden_size` | 前馈网络隐藏层维度，一般比 embed\\_size 大，比如 4\\~8 倍                 |\n",
    "| `dropout`        | Dropout 概率，用于防止过拟合                                    |\n",
    "| `max_length`     | 位置编码最大支持序列长度                                          |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757d0f0a",
   "metadata": {},
   "source": [
    "✅ 特点：\n",
    "\n",
    "- x 是 token id 序列，通过 embedding + position encoding 得到输入向量。\n",
    "- 每个 EncoderBlock 包含：\n",
    "  - 多头自注意力（Q/K 可降维，V 保持 embed_size）\n",
    "  - 前馈网络\n",
    "  - 残差 + LayerNorm\n",
    "- 可以堆叠多层，输出维度固定为 (batch, seq_len, embed_size)，便于传入 Decoder 或做下游任务。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea4d14f",
   "metadata": {},
   "source": [
    "## Decoder Block"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947500bb",
   "metadata": {},
   "source": [
    "输入 x  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;│  \n",
    "Masked Self-Attention (Q/K/V = x, mask = tgt_mask)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;│  \n",
    "残差 + LayerNorm  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;│  \n",
    "Cross-Attention (Q = x, K/V = enc_out, mask = src_mask)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;│  \n",
    "残差 + LayerNorm  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;│  \n",
    "Feed-Forward Network  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;│  \n",
    "残差 + LayerNorm  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;│  \n",
    "输出 x (N, tgt_seq_len, embed_size)\n",
    "\n",
    "好的，我们来仔细分析 Transformer Decoder 中的 **`tgt_mask`** 和 **`src_mask`** 的区别和作用。\n",
    "\n",
    "---\n",
    "\n",
    "### 1️⃣ `tgt_mask`（Target Mask / Decoder 自注意力掩码）\n",
    "\n",
    "**用途**：\n",
    "\n",
    "* 用于 **Masked Self-Attention**，屏蔽 Decoder 当前预测位置能看到的未来 token。\n",
    "* 目的是 **保证自回归（auto-regressive）生成** 的正确性：模型只能看到当前位置及之前的 token。\n",
    "\n",
    "**形式**：\n",
    "\n",
    "* 通常是一个 **下三角矩阵**（seq\\_len × seq\\_len），也可以广播成 `(N, 1, seq_len, seq_len)`\n",
    "* 对于位置 i，只能注意到 j ≤ i 的 token\n",
    "* 用在 `self_attention` 中的 mask 参数\n",
    "\n",
    "**示例（seq\\_len=4）**：\n",
    "\n",
    "| i\\j | 0 | 1 | 2 | 3 |\n",
    "| --- | - | - | - | - |\n",
    "| 0   | 1 | 0 | 0 | 0 |\n",
    "| 1   | 1 | 1 | 0 | 0 |\n",
    "| 2   | 1 | 1 | 1 | 0 |\n",
    "| 3   | 1 | 1 | 1 | 1 |\n",
    "\n",
    "* 1 表示允许注意\n",
    "* 0 表示屏蔽未来\n",
    "\n",
    "**作用**：确保生成时 **不泄露未来信息**。\n",
    "\n",
    "---\n",
    "\n",
    "### 2️⃣ `src_mask`（Source Mask / Encoder-Decoder 注意力掩码）\n",
    "\n",
    "**用途**：\n",
    "\n",
    "* 用于 **Cross-Attention**（Decoder 查询 Encoder 输出时）\n",
    "* 目的是 **屏蔽 Encoder 输入中的 padding token**，避免模型把无意义的 padding 当作有用信息\n",
    "\n",
    "**形式**：\n",
    "\n",
    "* 形状 `(N, 1, 1, src_seq_len)` 或 `(N, 1, tgt_seq_len, src_seq_len)`\n",
    "* 对应 Encoder 输入的每个 token，1 表示可注意，0 表示屏蔽\n",
    "\n",
    "**示例（src\\_seq\\_len=5, padding 在最后两个位置）**：\n",
    "\n",
    "| token 0 | token 1 | token 2 | token 3 | token 4 |\n",
    "| ------- | ------- | ------- | ------- | ------- |\n",
    "| 1       | 1       | 1       | 0       | 0       |\n",
    "\n",
    "* Decoder 在做 cross-attention 时，不会关注 padding\n",
    "\n",
    "**作用**：避免模型学习无意义的 padding 信息，提升训练效果。\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 总结对比\n",
    "\n",
    "| Mask 类型    | 使用场景                              | 屏蔽目标                  | 作用                   |\n",
    "| ---------- | --------------------------------- | --------------------- | -------------------- |\n",
    "| `tgt_mask` | Decoder 自注意力                      | 未来位置                  | 保证自回归生成，只依赖已生成 token |\n",
    "| `src_mask` | Decoder → Encoder Cross-Attention | Encoder padding token | 避免关注无效信息，提高注意力效率     |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05d8db18",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, embed_size, heads, qk_dim, ff_hidden_size, dropout=0.1):\n",
    "        super().__init__()\n",
    "        # 1️⃣ Masked Self-Attention（Decoder 内部自注意力）\n",
    "        self.self_attention = SelfAttention(embed_size, heads, qk_dim)\n",
    "        self.norm1 = nn.LayerNorm(embed_size)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "\n",
    "        # 2️⃣ Cross-Attention（Decoder 查询 Encoder 输出）\n",
    "        self.cross_attention = SelfAttention(embed_size, heads, qk_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_size)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "        # 3️⃣ Feed-Forward Network\n",
    "        self.feed_forward = FeedForward(embed_size, ff_hidden_size, dropout)\n",
    "        self.norm3 = nn.LayerNorm(embed_size)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, enc_out, src_mask=None, tgt_mask=None):\n",
    "        \"\"\"\n",
    "        x:        (N, tgt_seq_len, embed_size)  Decoder 输入\n",
    "        enc_out:  (N, src_seq_len, embed_size)  Encoder 输出\n",
    "        src_mask: (N, 1, 1, src_seq_len)        Encoder-Decoder 注意力 mask\n",
    "        tgt_mask: (N, 1, tgt_seq_len, tgt_seq_len)  Decoder 自注意力 mask\n",
    "        \"\"\"\n",
    "\n",
    "        # -----------------------------\n",
    "        # 1️⃣ Masked Self-Attention\n",
    "        # Q/K/V 都来自 x\n",
    "        # tgt_mask 用于屏蔽未来信息（下三角 mask）\n",
    "        # 输出维度: (N, tgt_seq_len, embed_size)\n",
    "        # -----------------------------\n",
    "        _x = self.self_attention(query=x, key=None, value=None, mask=tgt_mask)\n",
    "        x = self.norm1(x + self.dropout1(_x))  # 残差连接 + LayerNorm\n",
    "\n",
    "        # -----------------------------\n",
    "        # 2️⃣ Cross-Attention\n",
    "        # Q 来自 Decoder 当前输入 x\n",
    "        # K/V 来自 Encoder 输出 enc_out\n",
    "        # src_mask 用于屏蔽 padding token\n",
    "        # 输出维度: (N, tgt_seq_len, embed_size)\n",
    "        # -----------------------------\n",
    "        _x = self.cross_attention(query=x, key=enc_out, value=enc_out, mask=src_mask)\n",
    "        x = self.norm2(x + self.dropout2(_x))  # 残差连接 + LayerNorm\n",
    "\n",
    "        # -----------------------------\n",
    "        # 3️⃣ Feed-Forward Network\n",
    "        # 输出维度: (N, tgt_seq_len, embed_size)\n",
    "        # -----------------------------\n",
    "        _x = self.feed_forward(x)\n",
    "        x = self.norm3(x + self.dropout3(_x))  # 残差连接 + LayerNorm\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a63a84",
   "metadata": {},
   "source": [
    "## Transformer Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62e04838",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, num_layers, heads, qk_dim,\n",
    "                 ff_hidden_size, dropout=0.1, max_length=100):\n",
    "        super().__init__()\n",
    "        self.embed_size = embed_size\n",
    "\n",
    "        # 1️⃣ 词嵌入层，将 token id 转换为向量\n",
    "        self.word_embedding = nn.Embedding(vocab_size, embed_size)\n",
    "\n",
    "        # 2️⃣ 位置编码，加入位置信息\n",
    "        self.position_encoding = PositionalEncoding(embed_size, max_length)\n",
    "\n",
    "        # 3️⃣ 多层 DecoderBlock\n",
    "        self.layers = nn.ModuleList([\n",
    "            DecoderBlock(embed_size, heads, qk_dim, ff_hidden_size, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        # 4️⃣ 输入 dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # 5️⃣ 输出投影层，将 embed_size 映射到 vocab_size\n",
    "        self.fc_out = nn.Linear(embed_size, vocab_size)\n",
    "\n",
    "    def forward(self, x, enc_out, src_mask=None, tgt_mask=None):\n",
    "        \"\"\"\n",
    "        x: (N, tgt_seq_len) 目标序列 token id\n",
    "        enc_out: (N, src_seq_len, embed_size) Encoder 输出\n",
    "        src_mask: (N, 1, 1, src_seq_len) Encoder padding mask\n",
    "        tgt_mask: (N, 1, tgt_seq_len, tgt_seq_len) Decoder 自注意力 mask\n",
    "        \"\"\"\n",
    "\n",
    "        # 1️⃣ token embedding + positional encoding\n",
    "        out = self.word_embedding(x)            # (N, tgt_seq_len, embed_size)\n",
    "        out = self.position_encoding(out)       # 加入位置信息\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        # 2️⃣ 多层 DecoderBlock 堆叠\n",
    "        for layer in self.layers:\n",
    "            out = layer(out, enc_out, src_mask, tgt_mask)\n",
    "            # out 维度保持 (N, tgt_seq_len, embed_size)\n",
    "\n",
    "        # 3️⃣ 输出投影到 vocab\n",
    "        out = self.fc_out(out)                  # (N, tgt_seq_len, vocab_size)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5b9862",
   "metadata": {},
   "source": [
    "有区别的，虽然在形式上都是 `(N, seq_len)` 的 token id，但 **含义和作用不同**：\n",
    "\n",
    "---\n",
    "\n",
    "### 1️⃣ Encoder 的输入 `x`\n",
    "\n",
    "* **类型**：源序列（source sequence）的 token id\n",
    "* **作用**：编码源序列信息，生成上下文向量 `enc_out`\n",
    "* **输入示例**：一段英文句子 `[The, cat, sits, .]` → `[101, 205, 330, 102]`\n",
    "* **特点**：\n",
    "\n",
    "  * 所有 token 都可用\n",
    "  * Encoder 通常不屏蔽未来 token\n",
    "  * 可加 `src_mask` 屏蔽 padding\n",
    "\n",
    "---\n",
    "\n",
    "### 2️⃣ Decoder 的输入 `x`\n",
    "\n",
    "* **类型**：目标序列（target sequence）的 token id\n",
    "* **作用**：\n",
    "\n",
    "  * 用于 **训练阶段**：做自回归预测，生成下一个 token\n",
    "  * 用于 **推理阶段**：从 `<SOS>` 开始，逐步生成 token\n",
    "* **输入示例**：翻译任务中目标句子 `[Le, chat, est, assis]` → `[101, 400, 500, 102]`\n",
    "* **特点**：\n",
    "\n",
    "  * 需要 **Masked Self-Attention** (`tgt_mask`) 避免看到未来 token\n",
    "  * Cross-Attention 查询 Encoder 输出（`enc_out`）\n",
    "\n",
    "---\n",
    "\n",
    "#### 🔹 总结\n",
    "\n",
    "| 模块      | 输入 `x` 类型                        | 作用                               |\n",
    "| ------- | -------------------------------- | -------------------------------- |\n",
    "| Encoder | 源序列 token id `(N, src_seq_len)`  | 编码源序列上下文，生成 `enc_out`            |\n",
    "| Decoder | 目标序列 token id `(N, tgt_seq_len)` | 自回归预测下一 token，结合 Encoder 上下文生成输出 |\n",
    "\n",
    "---\n",
    "\n",
    "如果你愿意，我可以画一张 **Encoder 和 Decoder 输入输出示意图**，直观显示 `x`、`enc_out`、`tgt_mask`、`src_mask` 之间的关系，这样理解会更清楚。\n",
    "\n",
    "你希望我画吗？\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd76b97f",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97fd9da0",
   "metadata": {},
   "source": [
    "src ──► Encoder ──► enc_out  \n",
    "tgt ──► Decoder(x = tgt, enc_out, src_mask, tgt_mask) ──► out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "174feabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, embed_size=256, num_layers=3,\n",
    "                 heads=8, qk_dim=64, ff_hidden_size=512, dropout=0.1, max_length=100):\n",
    "        \"\"\"\n",
    "        src_vocab_size: 源语言词表大小\n",
    "        tgt_vocab_size: 目标语言词表大小\n",
    "        embed_size: Embedding/模型维度 (d_model)\n",
    "        num_layers: Encoder/Decoder 层数\n",
    "        heads: 注意力头数\n",
    "        qk_dim: Q/K 投影维度\n",
    "        ff_hidden_size: 前馈网络隐藏层大小\n",
    "        dropout: Dropout 概率\n",
    "        max_length: 序列最大长度\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # 1️⃣ Encoder\n",
    "        self.encoder = TransformerEncoder(\n",
    "            vocab_size=src_vocab_size,\n",
    "            embed_size=embed_size,\n",
    "            num_layers=num_layers,\n",
    "            heads=heads,\n",
    "            qk_dim=qk_dim,\n",
    "            ff_hidden_size=ff_hidden_size,\n",
    "            dropout=dropout,\n",
    "            max_length=max_length\n",
    "        )\n",
    "\n",
    "        # 2️⃣ Decoder\n",
    "        self.decoder = TransformerDecoder(\n",
    "            vocab_size=tgt_vocab_size,\n",
    "            embed_size=embed_size,\n",
    "            num_layers=num_layers,\n",
    "            heads=heads,\n",
    "            qk_dim=qk_dim,\n",
    "            ff_hidden_size=ff_hidden_size,\n",
    "            dropout=dropout,\n",
    "            max_length=max_length\n",
    "        )\n",
    "\n",
    "    # -----------------------------\n",
    "    # 构造源序列 mask\n",
    "    # -----------------------------\n",
    "    def make_src_mask(self, src):\n",
    "        \"\"\"\n",
    "        src: (N, src_seq_len)\n",
    "        src_mask: (N, 1, 1, src_seq_len)\n",
    "        mask 为 1 的位置表示有效 token，为 0 的位置表示 padding\n",
    "        \"\"\"\n",
    "        return (src != 0).unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "    # -----------------------------\n",
    "    # 构造目标序列 mask\n",
    "    # -----------------------------\n",
    "    def make_tgt_mask(self, tgt):\n",
    "        \"\"\"\n",
    "        tgt: (N, tgt_seq_len)\n",
    "        tgt_mask: (N, 1, tgt_seq_len, tgt_seq_len)\n",
    "        mask 为下三角矩阵，防止 Decoder 看到未来 token\n",
    "        \"\"\"\n",
    "        N, tgt_len = tgt.shape\n",
    "        # 下三角矩阵\n",
    "        tgt_mask = torch.tril(torch.ones((tgt_len, tgt_len))).expand(N, 1, tgt_len, tgt_len)\n",
    "        return tgt_mask.to(tgt.device)\n",
    "\n",
    "    # -----------------------------\n",
    "    # 前向传播\n",
    "    # -----------------------------\n",
    "    def forward(self, src, tgt):\n",
    "        \"\"\"\n",
    "        src: (N, src_seq_len) 源序列 token id\n",
    "        tgt: (N, tgt_seq_len) 目标序列 token id\n",
    "        \"\"\"\n",
    "        # 1️⃣ 构造 mask\n",
    "        src_mask = self.make_src_mask(src)\n",
    "        tgt_mask = self.make_tgt_mask(tgt)\n",
    "\n",
    "        # 2️⃣ Encoder 编码源序列\n",
    "        enc_out = self.encoder(src, mask=src_mask)  # (N, src_seq_len, embed_size)\n",
    "\n",
    "        # 3️⃣ Decoder 解码目标序列\n",
    "        out = self.decoder(tgt, enc_out, src_mask=src_mask, tgt_mask=tgt_mask)\n",
    "        # 输出维度: (N, tgt_seq_len, tgt_vocab_size)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "318bfd4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src shape: torch.Size([2, 6])\n",
      "tgt shape: torch.Size([2, 6])\n",
      "模型输出 shape: torch.Size([2, 6, 1000])\n",
      "输出示例（部分）： [-0.2812156   0.4754691   0.96160126  0.35761005  0.6679771 ]\n"
     ]
    }
   ],
   "source": [
    "# 示例：构造输入并运行 Transformer\n",
    "\n",
    "import torch\n",
    "\n",
    "# 假设词表大小为 1000，源序列和目标序列长度均为 6\n",
    "src_vocab_size = 1000\n",
    "tgt_vocab_size = 1000\n",
    "src_seq_len = 6\n",
    "tgt_seq_len = 6\n",
    "batch_size = 2\n",
    "\n",
    "# 构造随机 token id（模拟输入）\n",
    "src = torch.randint(1, src_vocab_size, (batch_size, src_seq_len))\n",
    "tgt = torch.randint(1, tgt_vocab_size, (batch_size, tgt_seq_len))\n",
    "\n",
    "# 实例化模型\n",
    "model = Transformer(\n",
    "    src_vocab_size=src_vocab_size,\n",
    "    tgt_vocab_size=tgt_vocab_size,\n",
    "    embed_size=32,        # 可以调小以便快速实验\n",
    "    num_layers=2,\n",
    "    heads=4,\n",
    "    qk_dim=32,\n",
    "    ff_hidden_size=64,\n",
    "    dropout=0.1,\n",
    "    max_length=20\n",
    ")\n",
    "\n",
    "# 前向推理\n",
    "out = model(src, tgt)\n",
    "\n",
    "print(\"src shape:\", src.shape)\n",
    "print(\"tgt shape:\", tgt.shape)\n",
    "print(\"模型输出 shape:\", out.shape)  # (batch_size, tgt_seq_len, tgt_vocab_size)\n",
    "print(\"输出示例（部分）：\", out[0, 0, :5].detach().numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
